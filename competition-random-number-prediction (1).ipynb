{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":113115,"databundleVersionId":13478352,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:14.573290Z","iopub.execute_input":"2025-09-15T01:17:14.573531Z","iopub.status.idle":"2025-09-15T01:17:18.924830Z","shell.execute_reply.started":"2025-09-15T01:17:14.573506Z","shell.execute_reply":"2025-09-15T01:17:18.923040Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data acquired","metadata":{}},{"cell_type":"code","source":"\ndf_train=pd.read_csv(\"/kaggle/input/predicting-random-numbers/train.csv\")\ndf_test=pd.read_csv(\"/kaggle/input/predicting-random-numbers/test.csv\")\ndf_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:18.926553Z","iopub.execute_input":"2025-09-15T01:17:18.927717Z","iopub.status.idle":"2025-09-15T01:17:18.993742Z","shell.execute_reply.started":"2025-09-15T01:17:18.927673Z","shell.execute_reply":"2025-09-15T01:17:18.992832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:18.996127Z","iopub.execute_input":"2025-09-15T01:17:18.996493Z","iopub.status.idle":"2025-09-15T01:17:19.015421Z","shell.execute_reply.started":"2025-09-15T01:17:18.996460Z","shell.execute_reply":"2025-09-15T01:17:19.014344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Know your data","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.016597Z","iopub.execute_input":"2025-09-15T01:17:19.017057Z","iopub.status.idle":"2025-09-15T01:17:19.041309Z","shell.execute_reply.started":"2025-09-15T01:17:19.017028Z","shell.execute_reply":"2025-09-15T01:17:19.040372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#missing values\ndf_train[df_train.isnull().any(axis=1)] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.042167Z","iopub.execute_input":"2025-09-15T01:17:19.042494Z","iopub.status.idle":"2025-09-15T01:17:19.073165Z","shell.execute_reply.started":"2025-09-15T01:17:19.042464Z","shell.execute_reply":"2025-09-15T01:17:19.072044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df_train.describe(include='all'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.074231Z","iopub.execute_input":"2025-09-15T01:17:19.074591Z","iopub.status.idle":"2025-09-15T01:17:19.110832Z","shell.execute_reply.started":"2025-09-15T01:17:19.074542Z","shell.execute_reply":"2025-09-15T01:17:19.109825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.112014Z","iopub.execute_input":"2025-09-15T01:17:19.112337Z","iopub.status.idle":"2025-09-15T01:17:19.120269Z","shell.execute_reply.started":"2025-09-15T01:17:19.112305Z","shell.execute_reply":"2025-09-15T01:17:19.119268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_values = df_train['timestamp'].unique()\nprint(unique_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.121885Z","iopub.execute_input":"2025-09-15T01:17:19.122322Z","iopub.status.idle":"2025-09-15T01:17:19.140710Z","shell.execute_reply.started":"2025-09-15T01:17:19.122286Z","shell.execute_reply":"2025-09-15T01:17:19.139469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#unique_delays = df_train['timedelay'].unique()\n#print(unique_delays)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.144409Z","iopub.execute_input":"2025-09-15T01:17:19.144793Z","iopub.status.idle":"2025-09-15T01:17:19.159349Z","shell.execute_reply.started":"2025-09-15T01:17:19.144741Z","shell.execute_reply":"2025-09-15T01:17:19.158324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"uniques = df_train['name'].unique()\nprint(uniques)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.160409Z","iopub.execute_input":"2025-09-15T01:17:19.160737Z","iopub.status.idle":"2025-09-15T01:17:19.180124Z","shell.execute_reply.started":"2025-09-15T01:17:19.160709Z","shell.execute_reply":"2025-09-15T01:17:19.179165Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Clean the data for predicting number","metadata":{}},{"cell_type":"code","source":"#remove the rows with NaN in columns\" name or number\" both.\ndf_train = df_train.dropna(subset=['name', 'number'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.181018Z","iopub.execute_input":"2025-09-15T01:17:19.181306Z","iopub.status.idle":"2025-09-15T01:17:19.200134Z","shell.execute_reply.started":"2025-09-15T01:17:19.181267Z","shell.execute_reply":"2025-09-15T01:17:19.199196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#missing values\ndf_train[df_train.isnull().any(axis=1)] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.201271Z","iopub.execute_input":"2025-09-15T01:17:19.201596Z","iopub.status.idle":"2025-09-15T01:17:19.229014Z","shell.execute_reply.started":"2025-09-15T01:17:19.201564Z","shell.execute_reply":"2025-09-15T01:17:19.227824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.230192Z","iopub.execute_input":"2025-09-15T01:17:19.230699Z","iopub.status.idle":"2025-09-15T01:17:19.255639Z","shell.execute_reply.started":"2025-09-15T01:17:19.230657Z","shell.execute_reply":"2025-09-15T01:17:19.254634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['number'].value_counts().plot(kind='bar')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.256732Z","iopub.execute_input":"2025-09-15T01:17:19.257038Z","iopub.status.idle":"2025-09-15T01:17:19.774431Z","shell.execute_reply.started":"2025-09-15T01:17:19.257019Z","shell.execute_reply":"2025-09-15T01:17:19.773321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let pandas try to automatically figure out the format\n# `errors='coerce'` will turn un-parsable timestamps into NaT (Not a Time)\ndf_train['timestamp_clean'] = pd.to_datetime(df_train['timestamp'], errors='coerce')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.775396Z","iopub.execute_input":"2025-09-15T01:17:19.775660Z","iopub.status.idle":"2025-09-15T01:17:19.861629Z","shell.execute_reply.started":"2025-09-15T01:17:19.775639Z","shell.execute_reply":"2025-09-15T01:17:19.860577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train[(df_train['timestamp_clean'].isnull())]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.862618Z","iopub.execute_input":"2025-09-15T01:17:19.862933Z","iopub.status.idle":"2025-09-15T01:17:19.877193Z","shell.execute_reply.started":"2025-09-15T01:17:19.862901Z","shell.execute_reply":"2025-09-15T01:17:19.876004Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['timestamp_clean']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.878024Z","iopub.execute_input":"2025-09-15T01:17:19.878318Z","iopub.status.idle":"2025-09-15T01:17:19.900919Z","shell.execute_reply.started":"2025-09-15T01:17:19.878295Z","shell.execute_reply":"2025-09-15T01:17:19.899861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.loc[df_train['name'] == 'Utpal_2301PH24']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.901981Z","iopub.execute_input":"2025-09-15T01:17:19.902292Z","iopub.status.idle":"2025-09-15T01:17:19.931393Z","shell.execute_reply.started":"2025-09-15T01:17:19.902265Z","shell.execute_reply":"2025-09-15T01:17:19.930443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.loc[df_train['name'] == 'av']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.932617Z","iopub.execute_input":"2025-09-15T01:17:19.933082Z","iopub.status.idle":"2025-09-15T01:17:19.958301Z","shell.execute_reply.started":"2025-09-15T01:17:19.933046Z","shell.execute_reply":"2025-09-15T01:17:19.957355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.loc[df_train['name'] == 'empty']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.959345Z","iopub.execute_input":"2025-09-15T01:17:19.959604Z","iopub.status.idle":"2025-09-15T01:17:19.982078Z","shell.execute_reply.started":"2025-09-15T01:17:19.959583Z","shell.execute_reply":"2025-09-15T01:17:19.980994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import datetime\nimport pytz\n\n# Your timestamp\ntimestamp = 1754205926\n\n# Convert to UTC datetime\ndt_utc = datetime.datetime.utcfromtimestamp(timestamp)\n\n# Convert to your local timezone (for example, Asia/Kolkata)\nlocal_tz = pytz.timezone('Asia/Kolkata')  # Use appropriate timezone\ndt_local = dt_utc.replace(tzinfo=pytz.utc).astimezone(local_tz)\n\nprint(\"UTC datetime:\", dt_utc)\nprint(\"Local datetime:\", dt_local)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:19.983167Z","iopub.execute_input":"2025-09-15T01:17:19.983426Z","iopub.status.idle":"2025-09-15T01:17:20.046738Z","shell.execute_reply.started":"2025-09-15T01:17:19.983405Z","shell.execute_reply":"2025-09-15T01:17:20.045166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport datetime\nimport pytz\n\n# Your timestamp\ntimestamp = 1753871403\t\n\n# Convert to UTC datetime\ndt_utc = datetime.datetime.utcfromtimestamp(timestamp)\n\n# Convert to your local timezone (for example, Asia/Kolkata)\nlocal_tz = pytz.timezone('Asia/Kolkata')  # Use appropriate timezone\ndt_local = dt_utc.replace(tzinfo=pytz.utc).astimezone(local_tz)\n\nprint(\"UTC datetime:\", dt_utc)\nprint(\"Local datetime:\", dt_local)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:20.047955Z","iopub.execute_input":"2025-09-15T01:17:20.048624Z","iopub.status.idle":"2025-09-15T01:17:20.055210Z","shell.execute_reply.started":"2025-09-15T01:17:20.048589Z","shell.execute_reply":"2025-09-15T01:17:20.054031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import datetime\nimport pytz\n\nlocal_tz = pytz.timezone('Asia/Kolkata')\n\ndef convert_to_local(ts_str):\n    ts = int(ts_str)\n    dt_utc = datetime.datetime.utcfromtimestamp(ts)\n    dt_local = dt_utc.replace(tzinfo=pytz.utc).astimezone(local_tz)\n    return dt_local.strftime('%Y-%m-%d %H:%M:%S')\n\n\n\npattern = r'^\\d{10}$'\n\n# Create a boolean mask safely by filling NaNs with empty string first\nmask = df_train['timestamp'].fillna('').str.match(pattern)\n\n# Use the mask to create/assign new column after conversion\ndf_train.loc[mask, 'timestamp_clean'] = df_train.loc[mask, 'timestamp'].apply(convert_to_local)\n\nprint(df_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:20.056250Z","iopub.execute_input":"2025-09-15T01:17:20.056730Z","iopub.status.idle":"2025-09-15T01:17:20.088982Z","shell.execute_reply.started":"2025-09-15T01:17:20.056699Z","shell.execute_reply":"2025-09-15T01:17:20.087861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train[(df_train['timestamp_clean'].isnull())]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:20.090131Z","iopub.execute_input":"2025-09-15T01:17:20.090721Z","iopub.status.idle":"2025-09-15T01:17:20.124619Z","shell.execute_reply.started":"2025-09-15T01:17:20.090680Z","shell.execute_reply":"2025-09-15T01:17:20.123486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:20.125620Z","iopub.execute_input":"2025-09-15T01:17:20.125918Z","iopub.status.idle":"2025-09-15T01:17:20.146236Z","shell.execute_reply.started":"2025-09-15T01:17:20.125887Z","shell.execute_reply":"2025-09-15T01:17:20.145254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['timedelay'] = pd.to_numeric(df_train['timedelay'], errors='coerce')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:20.148101Z","iopub.execute_input":"2025-09-15T01:17:20.148817Z","iopub.status.idle":"2025-09-15T01:17:20.169893Z","shell.execute_reply.started":"2025-09-15T01:17:20.148764Z","shell.execute_reply":"2025-09-15T01:17:20.168645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"min_delay = df_train['timedelay'].min()\nmax_delay = df_train['timedelay'].max()\nprint(f\"Timedelay ranges from {min_delay} to {max_delay}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:20.175286Z","iopub.execute_input":"2025-09-15T01:17:20.176213Z","iopub.status.idle":"2025-09-15T01:17:20.192164Z","shell.execute_reply.started":"2025-09-15T01:17:20.176177Z","shell.execute_reply":"2025-09-15T01:17:20.190950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.loc[(df_train['timedelay'] >= 100)]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:20.193128Z","iopub.execute_input":"2025-09-15T01:17:20.193455Z","iopub.status.idle":"2025-09-15T01:17:20.223455Z","shell.execute_reply.started":"2025-09-15T01:17:20.193425Z","shell.execute_reply":"2025-09-15T01:17:20.222380Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.drop(df_train[df_train['timedelay'] >= 100].index, inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:20.224539Z","iopub.execute_input":"2025-09-15T01:17:20.224917Z","iopub.status.idle":"2025-09-15T01:17:20.236438Z","shell.execute_reply.started":"2025-09-15T01:17:20.224888Z","shell.execute_reply":"2025-09-15T01:17:20.235453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:20.237517Z","iopub.execute_input":"2025-09-15T01:17:20.237877Z","iopub.status.idle":"2025-09-15T01:17:20.270621Z","shell.execute_reply.started":"2025-09-15T01:17:20.237848Z","shell.execute_reply":"2025-09-15T01:17:20.269565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['number'] = pd.to_numeric(df_train['number'], errors='coerce')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:20.271676Z","iopub.execute_input":"2025-09-15T01:17:20.272016Z","iopub.status.idle":"2025-09-15T01:17:20.294115Z","shell.execute_reply.started":"2025-09-15T01:17:20.271992Z","shell.execute_reply":"2025-09-15T01:17:20.292850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df_train.describe(include='all'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:20.295360Z","iopub.execute_input":"2025-09-15T01:17:20.295729Z","iopub.status.idle":"2025-09-15T01:17:20.328197Z","shell.execute_reply.started":"2025-09-15T01:17:20.295698Z","shell.execute_reply":"2025-09-15T01:17:20.327218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:20.329268Z","iopub.execute_input":"2025-09-15T01:17:20.329617Z","iopub.status.idle":"2025-09-15T01:17:20.343161Z","shell.execute_reply.started":"2025-09-15T01:17:20.329586Z","shell.execute_reply":"2025-09-15T01:17:20.342118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find names that appear exactly once\nunique_names = df_train['name'].value_counts()[df_train['name'].value_counts() == 1].index\n\n# Replace those unique names with 'anonymous'\ndf_train.loc[df_train['name'].isin(unique_names), 'name'] = 'anonymous'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:20.344204Z","iopub.execute_input":"2025-09-15T01:17:20.344739Z","iopub.status.idle":"2025-09-15T01:17:20.365646Z","shell.execute_reply.started":"2025-09-15T01:17:20.344705Z","shell.execute_reply":"2025-09-15T01:17:20.364526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:20.366686Z","iopub.execute_input":"2025-09-15T01:17:20.367028Z","iopub.status.idle":"2025-09-15T01:17:20.394561Z","shell.execute_reply.started":"2025-09-15T01:17:20.367006Z","shell.execute_reply":"2025-09-15T01:17:20.393417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.loc[df_train['name'] == 'anonymous']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:17:20.395803Z","iopub.execute_input":"2025-09-15T01:17:20.396080Z","iopub.status.idle":"2025-09-15T01:17:20.423966Z","shell.execute_reply.started":"2025-09-15T01:17:20.396059Z","shell.execute_reply":"2025-09-15T01:17:20.422906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nimport datetime\nimport pytz\nimport sys\n\n# =============================================================================\n# STEP 1 & 2: LOAD DATA AND LEARN PARAMETERS\n# =============================================================================\nprint(\"Step 1 & 2: Loading data and learning parameters...\")\ntry:\n    df_train_raw = pd.read_csv(\"/kaggle/input/predicting-random-numbers/train.csv\")\n    df_test_raw = pd.read_csv(\"/kaggle/input/predicting-random-numbers/test.csv\")\nexcept FileNotFoundError as e:\n    sys.exit(f\"ERROR: File not found. Please check the path: {e.filename}\")\n\nmedian_timedelay = pd.to_numeric(df_train_raw['timedelay'], errors='coerce').median()\nmode_timestamp = pd.to_datetime(df_train_raw['timestamp'], errors='coerce').mode()[0]\nname_freq_map = df_train_raw['name'].value_counts().to_dict()\nunique_names_to_replace = df_train_raw['name'].value_counts()[df_train_raw['name'].value_counts() == 1].index\n\n# =============================================================================\n# STEP 3: MASTER PROCESSING FUNCTION\n# =============================================================================\nprint(\"Step 3: Building and applying the master processing function...\")\n\ndef convert_to_local(ts_str):\n    local_tz = pytz.timezone('Asia/Kolkata')\n    ts = int(ts_str)\n    dt_utc = datetime.datetime.utcfromtimestamp(ts)\n    dt_local = dt_utc.replace(tzinfo=pytz.utc).astimezone(local_tz)\n    return dt_local.replace(tzinfo=None)\n\ndef process_data(df, unique_names_rules, median_delay_rule, mode_ts_rule, name_freq_rules):\n    df_processed = df.copy()\n    df_processed['name'] = df_processed['name'].fillna('anonymous')\n    df_processed.loc[df_processed['name'].isin(unique_names_rules), 'name'] = 'anonymous'\n    df_processed['timestamp_clean'] = pd.to_datetime(df_processed['timestamp'], errors='coerce')\n    pattern = r'^\\d{10}$'\n    mask = df_processed['timestamp'].fillna('').str.match(pattern)\n    df_processed.loc[mask, 'timestamp_clean'] = df_processed.loc[mask, 'timestamp'].apply(convert_to_local)\n    df_processed['timestamp_clean'] = df_processed['timestamp_clean'].fillna(mode_ts_rule)\n    df_processed['timedelay'] = pd.to_numeric(df_processed['timedelay'], errors='coerce')\n    df_processed['timedelay'] = df_processed['timedelay'].fillna(median_delay_rule)\n    df_processed['hour'] = df_processed['timestamp_clean'].dt.hour\n    df_processed['day_of_week'] = df_processed['timestamp_clean'].dt.dayofweek\n    df_processed['hour_sin'] = np.sin(2 * np.pi * df_processed['hour']/24)\n    df_processed['hour_cos'] = np.cos(2 * np.pi * df_processed['hour']/24)\n    df_processed['timedelay_log'] = np.log1p(df_processed['timedelay'])\n    df_processed['name_freq'] = df_processed['name'].map(name_freq_rules).fillna(1)\n    df_processed['is_anonymous'] = (df_processed['name'].str.lower() == 'anonymous').astype(int)\n    return df_processed\n\ntrain_df = process_data(df_train_raw, unique_names_to_replace, median_timedelay, mode_timestamp, name_freq_map)\ntest_df = process_data(df_test_raw, unique_names_to_replace, median_timedelay, mode_timestamp, name_freq_map)\n\n# =============================================================================\n# STEP 4: CLEAN TARGET AND APPLY FILTERS\n# =============================================================================\nprint(\"Step 4: Cleaning target variable and filtering training data...\")\n\ntrain_df['number'] = pd.to_numeric(df_train_raw['number'], errors='coerce')\ntrain_df.dropna(subset=['number'], inplace=True)\ntrain_df['number'] = train_df['number'].astype(int)\ntrain_df = train_df[train_df['timedelay'] < 100].copy()\n\n# =============================================================================\n# STEP 5: CREATE ADVANCED FEATURES (WITH ROBUST SEPARATION)\n# =============================================================================\nprint(\"Step 5: Creating advanced features on clean data...\")\n\n# CORRECTED LOGIC: Add a 'source' column to tag the data BEFORE combining\ntrain_df['source'] = 'train'\ntest_df['source'] = 'test'\n\ncombined_df = pd.concat([train_df, test_df], sort=False, ignore_index=True)\ncombined_df.sort_values(['name', 'timestamp_clean'], inplace=True)\n\ncombined_df['lag1_number'] = combined_df.groupby('name')['number'].shift(1)\nagg_stats = combined_df.groupby('name')['timedelay_log'].agg(['mean', 'std'])\nagg_stats.columns = ['user_mean_delay', 'user_std_delay']\ncombined_df = combined_df.merge(agg_stats, on='name', how='left')\ncombined_df['delay_x_hour'] = combined_df['timedelay_log'] * combined_df['hour']\nfeature_cols_to_fill = ['lag1_number', 'user_mean_delay', 'user_std_delay']\nfor col in feature_cols_to_fill:\n    combined_df[col] = combined_df[col].fillna(-1)\n\n# CORRECTED LOGIC: Separate using the 'source' tag for 100% accuracy\ntrain_df_final = combined_df[combined_df['source'] == 'train'].drop(columns=['source'])\ntest_df_final = combined_df[combined_df['source'] == 'test'].drop(columns=['source'])\n\n# =============================================================================\n# STEP 6: MODEL TRAINING AND SUBMISSION\n# =============================================================================\nprint(\"Step 6: Starting model training and submission...\")\n\nfeatures_to_use = [\n    'hour_sin', 'hour_cos', 'timedelay_log', 'name_freq',\n    'is_anonymous', 'delay_x_hour', 'lag1_number',\n    'user_mean_delay', 'user_std_delay'\n]\n\nX = train_df_final[features_to_use]\ny = train_df_final['number'].astype(int) # Ensure y is int\nX_test = test_df_final[features_to_use]\n\nprint(\"\\nStarting 5-Fold Cross-Validation...\")\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfold_f1_scores = []\nfor fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n    model = lgb.LGBMClassifier(objective='multiclass', num_class=10, class_weight='balanced', random_state=42)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_val)\n    f1 = f1_score(y_val, preds, average='macro')\n    fold_f1_scores.append(f1)\n    print(f\"Fold {fold+1} Macro F1-Score: {f1:.4f}\")\nprint(f\"\\nMean Macro F1-Score across 5 folds: {np.mean(fold_f1_scores):.4f}\")\n\nprint(\"\\nTraining final model...\")\nfinal_model = lgb.LGBMClassifier(objective='multiclass', num_class=10, class_weight='balanced', random_state=42)\nfinal_model.fit(X, y)\ntest_predictions = final_model.predict(X_test)\n\nsubmission_df = pd.DataFrame({'id': test_df_final['id'], 'number': test_predictions})\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"\\nBhai, it is done! Congratulations!\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:27:04.440400Z","iopub.execute_input":"2025-09-15T01:27:04.440920Z","iopub.status.idle":"2025-09-15T01:27:09.225960Z","shell.execute_reply.started":"2025-09-15T01:27:04.440889Z","shell.execute_reply":"2025-09-15T01:27:09.225037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:27:57.627968Z","iopub.execute_input":"2025-09-15T01:27:57.628285Z","iopub.status.idle":"2025-09-15T01:27:57.638014Z","shell.execute_reply.started":"2025-09-15T01:27:57.628259Z","shell.execute_reply":"2025-09-15T01:27:57.637017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nimport datetime\nimport pytz\nimport sys\n\n# =============================================================================\n# STEP 1: LOAD YOUR ORIGINAL RAW DATA\n# =============================================================================\nprint(\"Step 1: Loading original raw data...\")\ntry:\n    df_train_raw = pd.read_csv(\"/kaggle/input/predicting-random-numbers/train.csv\")\n    df_test_raw = pd.read_csv(\"/kaggle/input/predicting-random-numbers/test.csv\")\nexcept FileNotFoundError as e:\n    sys.exit(f\"ERROR: File not found. Please check the path: {e.filename}\")\n\n# =============================================================================\n# STEP 2: LEARN PARAMETERS FROM TRAINING DATA ONLY\n# =============================================================================\nprint(\"Step 2: Learning imputation parameters from training data...\")\nmedian_timedelay = pd.to_numeric(df_train_raw['timedelay'], errors='coerce').median()\nmode_timestamp = pd.to_datetime(df_train_raw['timestamp'], errors='coerce').mode()[0]\nname_freq_map = df_train_raw['name'].value_counts().to_dict()\nunique_names_to_replace = df_train_raw['name'].value_counts()[df_train_raw['name'].value_counts() == 1].index\n\n# =============================================================================\n# STEP 3: CREATE THE MASTER PROCESSING FUNCTION\n# =============================================================================\nprint(\"Step 3: Building and applying the master processing function...\")\n\ndef convert_to_local(ts_str):\n    local_tz = pytz.timezone('Asia/Kolkata')\n    ts = int(ts_str)\n    dt_utc = datetime.datetime.utcfromtimestamp(ts)\n    dt_local = dt_utc.replace(tzinfo=pytz.utc).astimezone(local_tz)\n    return dt_local.replace(tzinfo=None)\n\ndef process_data(df):\n    df_processed = df.copy()\n    df_processed['name'] = df_processed['name'].fillna('anonymous')\n    df_processed.loc[df_processed['name'].isin(unique_names_to_replace), 'name'] = 'anonymous'\n    df_processed['timestamp_clean'] = pd.to_datetime(df_processed['timestamp'], errors='coerce')\n    pattern = r'^\\d{10}$'\n    mask = df_processed['timestamp'].fillna('').str.match(pattern)\n    df_processed.loc[mask, 'timestamp_clean'] = df_processed.loc[mask, 'timestamp'].apply(convert_to_local)\n    df_processed['timestamp_clean'] = df_processed['timestamp_clean'].fillna(mode_timestamp)\n    df_processed['timedelay'] = pd.to_numeric(df_processed['timedelay'], errors='coerce')\n    df_processed['timedelay'] = df_processed['timedelay'].fillna(median_timedelay)\n    df_processed['hour'] = df_processed['timestamp_clean'].dt.hour\n    df_processed['day_of_week'] = df_processed['timestamp_clean'].dt.dayofweek\n    df_processed['hour_sin'] = np.sin(2 * np.pi * df_processed['hour']/24)\n    df_processed['hour_cos'] = np.cos(2 * np.pi * df_processed['hour']/24)\n    df_processed['timedelay_log'] = np.log1p(df_processed['timedelay'])\n    df_processed['name_freq'] = df_processed['name'].map(name_freq_map).fillna(1)\n    df_processed['is_anonymous'] = (df_processed['name'].str.lower() == 'anonymous').astype(int)\n    return df_processed\n\ntrain_df = process_data(df_train_raw)\ntest_df = process_data(df_test_raw)\n\n# =============================================================================\n# STEP 4: CLEAN TARGET AND APPLY FILTERS\n# =============================================================================\nprint(\"Step 4: Cleaning target variable and filtering training data...\")\ntrain_df['number'] = pd.to_numeric(train_df['number'], errors='coerce')\ntrain_df.dropna(subset=['number'], inplace=True)\ntrain_df['number'] = train_df['number'].astype(int)\ntrain_df = train_df[train_df['timedelay'] < 100].copy()\n\n# =============================================================================\n# STEP 5: CREATE ADVANCED FEATURES\n# =============================================================================\nprint(\"Step 5: Creating advanced features...\")\ntrain_df['source'] = 'train'\ntest_df['source'] = 'test'\ncombined_df = pd.concat([train_df, test_df], sort=False, ignore_index=True)\ncombined_df.sort_values(['name', 'timestamp_clean'], inplace=True)\ncombined_df['lag1_number'] = combined_df.groupby('name')['number'].shift(1)\nagg_stats = combined_df.groupby('name')['timedelay_log'].agg(['mean', 'std'])\nagg_stats.columns = ['user_mean_delay', 'user_std_delay']\ncombined_df = combined_df.merge(agg_stats, on='name', how='left')\ncombined_df['delay_x_hour'] = combined_df['timedelay_log'] * combined_df['hour']\nfeature_cols_to_fill = ['lag1_number', 'user_mean_delay', 'user_std_delay']\nfor col in feature_cols_to_fill:\n    combined_df[col] = combined_df[col].fillna(-1)\n\ntrain_df_final = combined_df[combined_df['source'] == 'train'].drop(columns=['source'])\ntest_df_final = combined_df[combined_df['source'] == 'test'].drop(columns=['source'])\n\n# =============================================================================\n# STEP 6: MODEL TRAINING AND SUBMISSION (\"PARANOID MODE\")\n# =============================================================================\nprint(\"Step 6: Starting model training and submission...\")\nfeatures_to_use = [\n    'hour_sin', 'hour_cos', 'timedelay_log', 'name_freq',\n    'is_anonymous', 'delay_x_hour', 'lag1_number',\n    'user_mean_delay', 'user_std_delay'\n]\nX = train_df_final[features_to_use]\ny = train_df_final['number'].astype(int)\nX_test = test_df_final[features_to_use]\n\nprint(\"\\nStarting 5-Fold Cross-Validation...\")\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfold_f1_scores = []\nfor fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n    model = lgb.LGBMClassifier(objective='multiclass', num_class=10, class_weight='balanced', random_state=42)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_val)\n    f1 = f1_score(y_val, preds, average='macro')\n    fold_f1_scores.append(f1)\nprint(f\"\\nMean Macro F1-Score across 5 folds: {np.mean(fold_f1_scores):.4f}\")\n\nprint(\"\\nTraining final model...\")\nfinal_model = lgb.LGBMClassifier(objective='multiclass', num_class=10, class_weight='balanced', random_state=42)\nfinal_model.fit(X, y)\ntest_predictions = final_model.predict(X_test)\n\nprint(\"\\nCreating the submission file with guaranteed integer types...\")\nsubmission_df = pd.DataFrame({'id': test_df_final['id'], 'number': test_predictions})\n# SAFETY CHECK 1: Force the 'number' column to be an integer.\nsubmission_df['number'] = submission_df['number'].astype(int)\n# SAFETY CHECK 2: Force the 'id' column to be an integer.\nsubmission_df['id'] = submission_df['id'].astype(int)\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"\\nBhai, it is done! Congratulations!\")\nprint(\"This submission file has its data types double-checked.\")\nprint(submission_df.head())\nprint(\"\\nFinal data types of submission file:\")\nprint(submission_df.dtypes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T02:00:46.822717Z","iopub.execute_input":"2025-09-15T02:00:46.823197Z","iopub.status.idle":"2025-09-15T02:00:51.643302Z","shell.execute_reply.started":"2025-09-15T02:00:46.823164Z","shell.execute_reply":"2025-09-15T02:00:51.642263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:40:08.221399Z","iopub.execute_input":"2025-09-15T01:40:08.222449Z","iopub.status.idle":"2025-09-15T01:40:08.233646Z","shell.execute_reply.started":"2025-09-15T01:40:08.222416Z","shell.execute_reply":"2025-09-15T01:40:08.232800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:52:47.526984Z","iopub.execute_input":"2025-09-15T01:52:47.527468Z","iopub.status.idle":"2025-09-15T01:52:47.552146Z","shell.execute_reply.started":"2025-09-15T01:52:47.527434Z","shell.execute_reply":"2025-09-15T01:52:47.550729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport sys\n\nprint(\"--- RUNNING SCRIPT #1: THE SAFE BASELINE MODEL ---\")\n\n# =============================================================================\n# LOAD DATA\n# =============================================================================\ntry:\n    df_train_raw = pd.read_csv(\"/kaggle/input/predicting-random-numbers/train.csv\")\n    df_test_raw = pd.read_csv(\"/kaggle/input/predicting-random-numbers/test.csv\")\nexcept FileNotFoundError:\n    sys.exit(\"ERROR: Could not find the Kaggle input files. Please check the path.\")\n\n# =============================================================================\n# BASELINE LOGIC\n# =============================================================================\n# Clean the target variable to find the true mode\ndf_train_raw['number'] = pd.to_numeric(df_train_raw['number'], errors='coerce')\ndf_train_raw.dropna(subset=['number'], inplace=True)\ndf_train_raw['number'] = df_train_raw['number'].astype(int)\n\n# Find the single most common number\nmost_frequent_number = df_train_raw['number'].mode()[0]\nprint(f\"The most frequent number in the training data is: {most_frequent_number}\")\n\n# Create a submission dataframe\nsubmission_df = pd.DataFrame({'id': df_test_raw['id'], 'number': most_frequent_number})\n\n# Force the columns to be integers\nsubmission_df['id'] = submission_df['id'].astype(int)\nsubmission_df['number'] = submission_df['number'].astype(int)\n\n# =============================================================================\n# SAVE SUBMISSION\n# =============================================================================\nsubmission_df.to_csv('baseline_submission.csv', index=False)\nprint(\"\\n'baseline_submission.csv' has been created.\")\nprint(\"This is your safe submission.\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T02:07:50.920233Z","iopub.execute_input":"2025-09-15T02:07:50.920677Z","iopub.status.idle":"2025-09-15T02:07:50.950607Z","shell.execute_reply.started":"2025-09-15T02:07:50.920654Z","shell.execute_reply":"2025-09-15T02:07:50.948888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nimport datetime\nimport pytz\nimport sys\n\n# =============================================================================\n# STEPS 1-4 (These are correct and remain the same)\n# =============================================================================\nprint(\"Steps 1-4: Loading, Learning, Processing, and Cleaning...\")\ntry:\n    df_train_raw = pd.read_csv(\"/kaggle/input/predicting-random-numbers/train.csv\")\n    df_test_raw = pd.read_csv(\"/kaggle/input/predicting-random-numbers/test.csv\")\nexcept FileNotFoundError as e:\n    sys.exit(f\"ERROR: File not found. Please check the path: {e.filename}\")\n\nmedian_timedelay = pd.to_numeric(df_train_raw['timedelay'], errors='coerce').median()\nmode_timestamp = pd.to_datetime(df_train_raw['timestamp'], errors='coerce').mode()[0]\nname_freq_map = df_train_raw['name'].value_counts().to_dict()\nunique_names_to_replace = df_train_raw['name'].value_counts()[df_train_raw['name'].value_counts() == 1].index\n\ndef convert_to_local(ts_str):\n    local_tz = pytz.timezone('Asia/Kolkata')\n    ts = int(ts_str)\n    dt_utc = datetime.datetime.utcfromtimestamp(ts)\n    dt_local = dt_utc.replace(tzinfo=pytz.utc).astimezone(local_tz)\n    return dt_local.replace(tzinfo=None)\n\ndef process_data(df):\n    df_processed = df.copy()\n    df_processed['name'] = df_processed['name'].fillna('anonymous')\n    df_processed.loc[df_processed['name'].isin(unique_names_to_replace), 'name'] = 'anonymous'\n    df_processed['timestamp_clean'] = pd.to_datetime(df_processed['timestamp'], errors='coerce')\n    pattern = r'^\\d{10}$'\n    mask = df_processed['timestamp'].fillna('').str.match(pattern)\n    df_processed.loc[mask, 'timestamp_clean'] = df_processed.loc[mask, 'timestamp'].apply(convert_to_local)\n    df_processed['timestamp_clean'] = df_processed['timestamp_clean'].fillna(mode_timestamp)\n    df_processed['timedelay'] = pd.to_numeric(df_processed['timedelay'], errors='coerce')\n    df_processed['timedelay'] = df_processed['timedelay'].fillna(median_timedelay)\n    df_processed['hour'] = df_processed['timestamp_clean'].dt.hour\n    df_processed['day_of_week'] = df_processed['timestamp_clean'].dt.dayofweek\n    df_processed['hour_sin'] = np.sin(2 * np.pi * df_processed['hour']/24)\n    df_processed['hour_cos'] = np.cos(2 * np.pi * df_processed['hour']/24)\n    df_processed['timedelay_log'] = np.log1p(df_processed['timedelay'])\n    df_processed['name_freq'] = df_processed['name'].map(name_freq_map).fillna(1)\n    df_processed['is_anonymous'] = (df_processed['name'].str.lower() == 'anonymous').astype(int)\n    return df_processed\n\ntrain_df = process_data(df_train_raw)\ntest_df = process_data(df_test_raw)\n\ntrain_df['number'] = pd.to_numeric(df_train_raw['number'], errors='coerce')\ntrain_df.dropna(subset=['number'], inplace=True)\ntrain_df['number'] = train_df['number'].astype(int)\ntrain_df = train_df[train_df['timedelay'] < 100].copy()\n\n# =============================================================================\n# STEP 5: CREATE ADVANCED FEATURES (WITH ROBUST SEPARATION)\n# =============================================================================\nprint(\"Step 5: Creating advanced features...\")\ntrain_df['source'] = 'train'\ntest_df['source'] = 'test'\ncombined_df = pd.concat([train_df, test_df], sort=False, ignore_index=True)\ncombined_df.sort_values(['name', 'timestamp_clean'], inplace=True)\ncombined_df['lag1_number'] = combined_df.groupby('name')['number'].shift(1)\nagg_stats = combined_df.groupby('name')['timedelay_log'].agg(['mean', 'std'])\nagg_stats.columns = ['user_mean_delay', 'user_std_delay']\ncombined_df = combined_df.merge(agg_stats, on='name', how='left')\ncombined_df['delay_x_hour'] = combined_df['timedelay_log'] * combined_df['hour']\nfeature_cols_to_fill = ['lag1_number', 'user_mean_delay', 'user_std_delay']\nfor col in feature_cols_to_fill:\n    combined_df[col] = combined_df[col].fillna(-1)\n\ntrain_df_final = combined_df[combined_df['source'] == 'train'].drop(columns=['source'])\ntest_df_final = combined_df[combined_df['source'] == 'test'].drop(columns=['source'])\n\n# =============================================================================\n# STEP 6: MODEL TRAINING AND SUBMISSION\n# =============================================================================\nprint(\"Step 6: Starting model training and submission...\")\n\n# --- NEW FINAL SAFETY CHECK ---\n# Remove any rows from the final test set that have a missing ID\ntest_df_final.dropna(subset=['id'], inplace=True)\n\nfeatures_to_use = [\n    'hour_sin', 'hour_cos', 'timedelay_log', 'name_freq',\n    'is_anonymous', 'delay_x_hour', 'lag1_number',\n    'user_mean_delay', 'user_std_delay'\n]\nX = train_df_final[features_to_use]\ny = train_df_final['number'].astype(int)\nX_test = test_df_final[features_to_use] # X_test is now created from the clean test_df_final\n\n# --- Final Model Training ---\nprint(\"\\nTraining final model...\")\nfinal_model = lgb.LGBMClassifier(objective='multiclass', num_class=10, class_weight='balanced', random_state=42)\nfinal_model.fit(X, y)\ntest_predictions = final_model.predict(X_test)\n\n# --- APE Safety Fix ---\nmost_frequent_num = train_df_final['number'].mode()[0]\npredictions_series = pd.Series(test_predictions, index=X_test.index) # Ensure index alignment\npredictions_series = predictions_series.replace(0, most_frequent_num)\n\n# --- Create Final Submission ---\n# The 'id' column is now guaranteed to be clean\nsubmission_df = pd.DataFrame({'id': test_df_final['id'], 'number': predictions_series})\nsubmission_df['id'] = submission_df['id'].astype(int)\nsubmission_df['number'] = submission_df['number'].astype(int)\n\nsubmission_df.to_csv('advanced_submission.csv', index=False)\nprint(\"\\n'advanced_submission.csv' has been created.\")\nprint(\"This is your best shot submission.\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T02:10:04.821464Z","iopub.execute_input":"2025-09-15T02:10:04.821847Z","iopub.status.idle":"2025-09-15T02:10:05.870362Z","shell.execute_reply.started":"2025-09-15T02:10:04.821820Z","shell.execute_reply":"2025-09-15T02:10:05.869383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}